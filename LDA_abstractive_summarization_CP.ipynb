{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment:\n",
    "1. Move all libraries to the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Barbieri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import zipfile\n",
    "\n",
    "# Gensim\n",
    "#import gensim\n",
    "#import gensim.corpora as corpora\n",
    "#from gensim.utils import simple_preprocess\n",
    "#from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "#import spacy\n",
    "\n",
    "# Plotting tools\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "#import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "#import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImportingAbstractive Summarization Literature\n",
    "\n",
    "#### Comment\n",
    "1. Avoid \"+\" concatenation for directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip_files(path_to_zip_file, directory_to_extract_to):\n",
    "    with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "path_to_zip_file = os.path.join('abstractive_summ_academic_literature', 'txtAbstractiveSummarization.zip')\n",
    "directory_to_extract_to = cwd + '/abstractive_summ_academic_literature/'\n",
    "\n",
    "extract_zip_files(path_to_zip_file, directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "1. Avoid creating empty dataframes. Just create the dataframe when you have the data you want ready.\n",
    "2. Avoid string slicing when you can.\n",
    "3. Avoid multiple for-blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(corpus_path):\n",
    "    content = []\n",
    "    filenames = []\n",
    "\n",
    "    for f in os.listdir(corpus_path):\n",
    "        if os.path.splitext(f)[1] == '.txt': #f[-4:] == '.txt':\n",
    "            with open (corpus_path + f, \"r\") as myfile:\n",
    "                content.append(myfile.read())\n",
    "                filenames.append(f)\n",
    "\n",
    "    df = pd.DataFrame(data=zip(*[filenames, content], columns=['filenames','content']))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(corpus_path):\n",
    "    df = import_dataset(corpus_path)\n",
    "    \n",
    "    # Convert to list\n",
    "    data = df.content.values.tolist()\n",
    "\n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    # pretty prints the first document in form of a list. Keep in mind that the fact\n",
    "    # that elements of that list look like sentences, they are not.\n",
    "    # pprint is just formatting the full content of the document as a list of smaller strings\n",
    "    # pprint(data[:1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "corpus_path =  directory_to_extract_to + '/txtAbstractiveSummarization/'\n",
    "k = clean_data(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize words and clean-up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here we are tokenizing each document.\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# data_words is a list where each element is the tokenized document\n",
    "# print(len(data_words))\n",
    "\n",
    "# printing the first document of the list, tokenized\n",
    "# print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Bigram and Trigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=10, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Bigram Trigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An example on how to apply the trained phrases model to a new, unseen sentence.\n",
    "# bigram[['opinion', 'consensus', 'minors']] # output: opinion_consensus is a bigram\n",
    "# trigram[['opinion', 'consensus', 'minors']] # output: opinion_consensus as well but since there is\n",
    "                                              # not enough co-ocurrance of another word after this bigram\n",
    "                                              # it still is a bigram only\n",
    "# print(bigram[['long', 'short', 'term']])  # this creates a bigram of short_term\n",
    "# print(trigram[['long', 'short', 'term']]) # this creates a bigram of short_term only. There is not enough words to create\n",
    "                                          # a trigram\n",
    "\n",
    "\n",
    "\n",
    "# Q: What is the difference between Phrases and Phraser\n",
    "# A: The type of data is different (one is a Phrases and the other a Phraser).\n",
    "# print(type(bigram))\n",
    "# print(type(bigram_mod))\n",
    "\n",
    "# The bigrams are the words that at least appear 10 times together in the document\n",
    "# bi1 = bigram[data_words[0]]\n",
    "# bi2 = bigram_mod[data_words[0]]\n",
    "# if bi1 == bi2:\n",
    "#     print('nothing changes!')\n",
    "\n",
    "# See trigram example\n",
    "# The trigrams are three words co-ocurring together more at least 10 times in the model\n",
    "# tri1 = trigram_mod[bigram_mod[data_words[0]]]\n",
    "# tri2 = trigram[bigram_mod[data_words[0]]]\n",
    "# if tri1 == tri2:\n",
    "#     print('nothing changes!')\n",
    "\n",
    "# print(tri1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "nlp = en_core_web_sm.load(disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Dictionary:\n",
    "# Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# print(len(id2word)) # corpus has 14118 unique tokens\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "# word with their corresponding id\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "# print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build list of topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_topics(num_topics, corpus):\n",
    "    list_models=[]\n",
    "    for n in num_topics:\n",
    "        topic_name = 'lda_model_' + str(n)\n",
    "        topic_name = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=n, random_state=100, update_every=1,\n",
    "                                                     chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
    "        list_models.append(topic_name)\n",
    "    return list_models\n",
    "\n",
    "num_topics = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "corpus = #somelist\n",
    "models = build_topics(num_topics, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the topics in LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "# pprint(lda_model.print_topics())\n",
    "# doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Model Perplexity and Coherence Score for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_perplexity_coherence(models):\n",
    "    list_perplexity = []\n",
    "    list_coherence = []\n",
    "    \n",
    "    for model in models:\n",
    "        list_perplexity.append(model.log_perplexity(corpus))\n",
    "        coherence_model_lda = CoherenceModel(model=model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "        list_coherence.append(coherence_model_lda.get_coherence())\n",
    "    return list_perplexity, list_coherence\n",
    "\n",
    "x = calc_perplexity_coherence(models)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame(list(x)).transpose()\n",
    "df_metrics.columns = ['Perplexity','Coherence']\n",
    "df_metrics['Number of topics'] = num_topics\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphic of Number of Topics and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity needs to be as low as possible \n",
    "plt.plot( 'Number of topics', 'Perplexity', data=df_metrics, color='skyblue')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphic of Number of Topics and Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity needs to be as low as possible \n",
    "plt.plot( 'Number of topics', 'Coherence', data=df_metrics, color='orange')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most salient topic per file using results of Model of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num) + 1, round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    file_names = df['filename']\n",
    "    sent_topics_df = pd.concat([file_names,sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "    \n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=models[0], corpus=corpus, texts=texts)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Filename', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export to csv\n",
    "current_path=os.getcwd()\n",
    "df_dominant_topic.to_csv(current_path + '/dominant_topic_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting papers in different directories of Model of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that splits all the papers in different folders according to the topic it belongs to\n",
    "# choose just one model first\n",
    "import shutil\n",
    "path_txt_files = directory_to_extract_to + 'txtAbstractiveSummarization/'\n",
    "df_document_topic = df_dominant_topic[['Filename','Dominant_Topic']]\n",
    "\n",
    "def split_corp_per_topic(abs_dirname):\n",
    "    for index, row in df_document_topic.iterrows():  \n",
    "        sel_topic = int(row['Dominant_Topic'])\n",
    "        file_name = row['Filename']       \n",
    "        folder_topic = abs_dirname + str(sel_topic)\n",
    "        \n",
    "        if not os.path.exists(folder_topic):\n",
    "            os.makedirs(folder_topic)\n",
    "\n",
    "        from_folder = abs_dirname + file_name\n",
    "        to_folder = os.path.join(folder_topic, file_name)\n",
    "        shutil.move(from_folder,to_folder)\n",
    "\n",
    "        \n",
    "split_corp_per_topic(path_txt_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the topics-keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
